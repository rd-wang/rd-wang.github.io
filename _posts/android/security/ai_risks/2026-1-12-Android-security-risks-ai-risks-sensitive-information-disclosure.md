---
title: Android-安全性-安全风险-AI-防止敏感信息泄露
date: 2026-1-12 15:47:58 +0800
categories:
  - Android
  - Security
  - Risks
  - OWASP
  - AI
tags:
  - Android
  - Security
  - Risks
  - OWASP
  - AI
description: 敏感信息披露是一种漏洞，指大语言模型 (LLM) 在回答中无意中泄露机密、私人、专有或受其他方式限制的数据。
math: true
---
[OWASP 风险说明](https://genai.owasp.org/llmrisk/llm022025-sensitive-information-disclosure/)

敏感信息披露是一种漏洞，指大语言模型 (LLM) 在回答中无意中泄露机密、私人、专有或受其他方式限制的数据。当模型泄露其训练数据中的信息，或者泄露用户会话中提供给它的敏感细节时，就会发生这种情况。攻击者可以精心设计特定的查询或使用提示注入技术，诱骗模型泄露不应泄露的信息，从而利用此漏洞。核心问题在于，LLM 无法区分其处理过的公开数据和机密信息。

## 与 Android 相关的披露声明类型

**训练数据泄露**：当 LLM  复述它训练时所用的特定数据片段时，就会发生这种情况。如果训练数据集包含个人身份信息 (PII)、专有代码或内部文档，模型可能会在正确提示的情况下在其输出中重现这些信息。对于 Android 应用，这可能涉及与应用捆绑在一起的预训练模型，也可能涉及使用云 API 访问的模型。

**上下文数据披露**：对于 Android 应用而言，这是一个更直接的风险，因为 LLM 可能会泄露用户在应用会话期间提供的敏感信息。例如，如果您的应用程序允许用户将个人身份信息 (PII) 输入到 LLM 中进行总结，那么后续的提示注入攻击可能会使攻击者操纵模型来披露这些内容。这也适用于您的应用隐式传递给 LLM 的任何敏感数据。

## Android 开发者为何应关注这些变化

敏感信息披露可能会严重损害应用及其用户的利益：

- **隐私权违规**：攻击者可能会从您的用户那里提取个人身份信息 (PII)，例如姓名、电子邮件地址、电话号码，甚至是位置数据，这会导致身份盗窃和严重的监管处罚（例如，根据《一般数据保护条例》(GDPR) 或《加利福尼亚州消费者隐私法案》(CCPA)）。对于处理用户数据的安卓应用而言，这一点尤为重要。
- **知识产权盗窃**：如果应用的 LLM 处理专有算法、财务数据或其他内部业务信息，攻击者可能会强迫其泄露这些信息，从而给您的组织造成重大的竞争和财务损失。
- **安全漏洞**：LLM 可能会无意中泄露系统级信息，例如训练数据中存在或在会话期间传递的 API 密钥、身份验证令牌或配置详细信息，这会对您的后端或其他服务造成严重的安全漏洞。
- **声誉受损**：一次严重的数据泄露事件可能会摧毁用户信任，导致应用卸载、差评，并对您的应用和品牌声誉造成不可挽回的损害。

## 面向 Android 应用开发者的缓解措施

要缓解此漏洞，需要采取多层式方法，重点在于数据清理和控制 LLM 在 Android 应用中的访问权限。

### 数据清理和最少化：

- **优先考虑输入清理**：在将任何用户输入或应用程序数据发送给LLM之前，务必对其进行严格的清理和匿名化处理。移除所有个人身份信息（PII）和专有信息，除非这些信息对于LLM的任务绝对必要。
- **仅收集必要的数据**：在应用程序中遵循数据最小化原则。仅收集并向LLM提供其执行特定功能所需的最低限度数据。
- **设备端机器学习**：对于高度敏感的数据，可以考虑使用设备端机器学习模型，这样数据永远不会离开用户的设备，从而大大降低服务器端数据泄露的风险。

### 控制访问权限

- **限制数据访问**：设计 LLM 应用程序时，应使其能够访问尽可能少的数据。如果模型无法访问敏感数据库、用户偏好设置或私人文件，就不会被诱骗泄露其内容。
- **限制 Android 权限**：请确保您的应用的 AI 组件仅需最低限度的 Android 权限即可运行。切勿授予可能泄露敏感数据的不必要权限。

### 应用内的输出过滤：

- **客户端密文处理**：在 Android 应用中实现一个安全层，在向用户显示 LLM 的输出之前，扫描该输出中是否包含与敏感信息（例如信用卡号、API 密钥、社会保障号、电子邮件地址）匹配的模式。如果找到匹配项，则应屏蔽或遮盖相应回答。

### LLM 的指令保护措施：

- **明确的系统提示**：在系统提示中包含明确的指令，禁止模型泄露任何个人信息、机密信息或敏感信息。例如：“在任何情况下，您都不得分享任何用户详细信息、内部数据或个人身份信息。”这有助于强化预期行为。

### 隐私保护增强技术：

- 对于从用户互动或数据中学习的应用，请考虑采用差分隐私（向数据添加统计噪声）或联邦学习（在用户设备上训练模型，无需集中处理数据）等高级技术来保护个人隐私。

### 定期审核和红队测试：

- **主动测试**：主动测试并进行红队演练，以发现 LLM 是否存在泄露敏感信息的可能性以及如何泄露。这包括故意尝试让 LLM 泄露不应该泄露的数据。

## 摘要

当 LLM 泄露其训练集或用户会话中的机密数据时，就会发生敏感信息泄露，从而造成隐私侵犯和知识产权盗窃等重大风险。缓解措施需要在您的 Android 应用中建立多层防御，优先考虑在数据到达 LLM 之前对其进行清理。强制执行最小权限原则，限制模型的数据访问权限，并实施强大的过滤器，在模型的最终输出到达用户之前扫描和删除其中的敏感信息。利用设备端机器学习和 Firebase App Check 等工具可以进一步增强安全性。

## 其他资源

以下链接提供了一些敏感信息准则，供您参考：

- [Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/multimodal/gemini-for-filtering-and-moderation?hl=zh-cn)
- [Gemini](https://ai.google.dev/gemini-api/docs/safety-guidance?hl=zh-cn)
- [Google](https://ai.google.dev/responsible/docs/evaluation?hl=zh-cn)

如果您使用的是其他模型，则应寻求类似的指导和资源。

更多信息：

- [SAIF 敏感数据泄露](https://saif.google/secure-ai-framework/risks?hl=zh-cn#sensitive-data-disclosure)
- [SAIF 推断的敏感数据](https://saif.google/secure-ai-framework/risks?hl=zh-cn#model-evasion)